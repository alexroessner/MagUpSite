# DeckSiteAgent: Standalone Product Plan

## Confirmed Decisions (2025-02-11)

- **Architecture:** Incremental (PLAN.md). Templates stay in DeckSiteAgent, genericized in place. GEO 42 data moves to `examples/geo42/`. DeckSiteAgent stays functional at every step.
- **Three.js:** Optional, not always included. Generic templates work without it. Opt-in based on brand/data.
- **`docs/` directory:** Stale build artifacts (Eleventy outputs to `dist/`). Remove during cleanup.
- **`enrich.js` approach:** LLM infers complex fields (`roadmap`, `faqs`, `stats`, `methodology`, `challenges`, `case_studies`). `merge.js` stays deterministic for core fields. Enrichment is optional.

## Situation

Two products share one repo today:

1. **GEO 42 Site** — Live at `alexroessner.github.io/MagUpSite/`. Premium B2B SaaS landing page with Three.js, 23 pages, glassmorphism design. Done. Cannot be touched.

2. **DeckSiteAgent** — Multi-agent PDF-to-website pipeline. Personas, scripts, audit gates all designed. But 56% of templates are hardcoded to GEO 42 content, the pipeline has never run end-to-end, and CI force-pushes to MagUpSite on every commit.

**Risk right now:** Any push to `main` or `claude/*` rebuilds and force-pushes to MagUpSite, potentially breaking the live site.

---

## Phase 1: Freeze MagUpSite (Protect the Live Site)

**Goal:** MagUpSite repo becomes self-contained. DeckSiteAgent can never break it again.

### 1.1 Remove the cross-repo deployment from CI

In `.github/workflows/ci.yml`, remove:
- Lines 64-94: "Build for MagUpSite" + "Push to MagUpSite repo" steps
- Lines 150-154: MagUpSite smoke test
- The `MAGUPSITE_DEPLOY_TOKEN` secret references (v1/v2/v3)

The MagUpSite repo already has the built site on its `main` branch (force-pushed there by CI). GitHub Pages serves it from there. Once we stop pushing, it stays exactly as-is. No one else has write access. It's frozen.

### 1.2 Remove hardcoded MagUpSite references from DeckSiteAgent

| File | Line | Change |
|------|------|--------|
| `whitelabel.config.js` | 34 | `siteUrl: ""` (empty — set by env or per-project) |
| `scripts/merge.js` | 318 | `siteUrl: ""` (same) |

The templates already use `{{ site.url }}` dynamically — these are the only two places with hardcoded MagUpSite URLs.

### 1.3 Simplify CI to DeckSiteAgent-only

The remaining CI workflow:
- Build + lint (keep as-is, lines 1-62)
- Build for DeckSiteAgent Pages (keep, lines 96-108)
- Deploy to DeckSiteAgent Pages (keep, lines 110-134)
- Smoke test DeckSiteAgent only (simplify lines 136-148)

DeckSiteAgent gets its own GitHub Pages site at `alexroessner.github.io/DeckSiteAgent/` — this becomes the demo/docs site for the pipeline product.

---

## Phase 2: Extract GEO 42 Content into a Snapshot

**Goal:** Preserve the GEO 42 site as a reference "example project" without it polluting the generic templates.

### 2.1 Create `examples/geo42/` directory

Move all GEO 42-specific content into a self-contained example:

```
examples/geo42/
├── raw-extract.json          ← current data/raw-extract.json
├── scraped-styles.json       ← current data/scraped-styles.json
├── merged-blueprint.json     ← current data/merged-blueprint.json
├── whitelabel.config.js      ← current config (GEO 42 branded)
├── pageContent.json          ← current src/_data/pageContent.json
└── README.md                 ← documents this as a reference example
```

This serves two purposes:
1. Proof that the pipeline produces real sites (show don't tell)
2. Test fixture for the pipeline — run `npm run generate` against it and compare output

### 2.2 Create empty/starter data files

Replace the current data files with clean defaults:

- `data/raw-extract.json` → empty (generated by pipeline)
- `data/scraped-styles.json` → empty (generated by pipeline)
- `data/merged-blueprint.json` → empty (generated by pipeline)
- `src/_data/pageContent.json` → minimal schema with placeholder structure
- `whitelabel.config.js` → generic defaults (no brand name, neutral colors)

---

## Phase 3: Genericize Templates

**Goal:** Every template renders correctly from any `pageContent.json` produced by `merge.js`.

### 3.1 index.njk — the big one (1,185 lines, ~85% hardcoded)

Current state: Hardcoded typewriter text ("ChatGPT|Perplexity|Gemini"), robot vacuum case study, 6 Walls framework, specific KPIs, flywheel methodology.

**Approach:** Make every section data-driven from `pageContent.json`:

| Current Hardcoded Section | Data Source After |
|--------------------------|-------------------|
| Hero typewriter platforms | `pageContent.hero.typewriter_items[]` |
| SEO vs GEO comparison | `pageContent.custom_sections[]` (rendered generically) |
| Case study mockup | `pageContent.case_studies[0]` (optional section) |
| 6 Walls | `pageContent.challenges[]` (optional section) |
| KPI stats ("7 AI Agents", "+450%") | `pageContent.stats[]` |
| Flywheel ("Become the Answer"...) | `pageContent.methodology.steps[]` |
| Client logo marquee | Already data-driven (keep) |
| Services grid | Already data-driven (keep) |
| Industries | `pageContent.industries[]` (already exists in data) |
| Roadmap | `pageContent.roadmap[]` (already exists in data) |

Each section wrapped in `{% if pageContent.challenges | length %}...{% endif %}` so missing data = missing section, not broken template.

### 3.2 about.njk — "About GEO 42" → generic

- Title: `{{ pageContent.company.name }}` instead of "About GEO 42"
- Headline: `{{ pageContent.about.headline }}` instead of hardcoded
- Stats: Loop `pageContent.stats[]` instead of hardcoded "10+ AI Platforms"
- Team section: Already data-driven (keep)

### 3.3 services.njk — hardcoded slugs → dynamic

- Title: `{{ pageContent.services_page.title | default("Services") }}`
- Slug array (lines 18-26): Generate from `pageContent.services[].name` instead of hardcoded
- Icon assignment: Map icons by index or let services define their own

### 3.4 contact.njk — branded copy → templated

- Title: `"Contact {{ site.name }}"` instead of "Contact GEO 42"
- Headline: `{{ pageContent.contact.headline | default("Get in Touch") }}`
- Copy: `{{ pageContent.contact.description }}`

### 3.5 team.njk — minor fix

- Title: `"{{ site.name }} Team"` instead of "GEO 42 Delivery Team"
- Intro: `{{ pageContent.team_intro | default("") }}`

### 3.6 base.njk — FAQs and branding

- Line 17: `content="{{ site.name }}"` instead of "GEO 42"
- Lines 122-175: FAQs should come from `pageContent.faqs[]` not hardcoded
- Line 216: Logo aria-label from `site.name`

### 3.7 sections.njk — keyword matching

- Lines 19-27: Replace hardcoded section-title matching with a generic icon-assignment strategy (assign icons by section index or content type)
- Lines 60-69: Replace GEO-specific keyword matching with generic tag-based related-section logic

---

## Phase 4: Wire Personas Into the Pipeline

**Goal:** Close the 62% gap between what personas describe and what scripts do.

### The Integration Model

The personas are written as instructions for an AI agent. The pipeline should invoke an LLM at specific gates, passing the persona as system prompt and the current data as context. This is the key architectural decision:

```
scripts/extract-pdf.js      → deterministic (pdf-parse)
scripts/scrape-styles.js    → deterministic (puppeteer)
scripts/enrich.js [NEW]     → LLM-powered (persona-guided quality pass)
scripts/merge.js             → deterministic (data assembly)
scripts/audit.js             → hybrid (deterministic checks + LLM review)
```

### 4.1 New script: `scripts/enrich.js`

Post-extraction, pre-merge LLM enrichment pass. This is where personas become operational:

**Document Analyst pass:**
- Input: `raw-extract.json`
- Persona: `personas/document-analyst.md`
- Task: "Review this extraction. Identify missing sections, dropped content, structural issues. Generate a discrepancy report."
- Output: `data/extract-review.json` (corrections, warnings, confidence scores)

**Content Architect pass:**
- Input: `raw-extract.json` + `extract-review.json`
- Persona: `personas/content-architect.md`
- Task: "Design the information architecture. Map sections to pages. Identify missing pages implied by the content."
- Output: `data/content-map.json` (page structure, nav hierarchy, CTA assignments)

**Copywriter pass:**
- Input: `raw-extract.json` + `content-map.json`
- Persona: `personas/copywriter.md`
- Task: "Adapt this PDF content for web. Write meta descriptions, CTAs, headlines. Remove print artifacts."
- Output: Updates to `pageContent.json` (web-ready copy)

**Brand Interpreter pass:**
- Input: `scraped-styles.json`
- Persona: `personas/brand-interpreter.md`
- Task: "Analyze this design system. Validate contrast, coherence, accessibility. Recommend adjustments."
- Output: `data/brand-review.json` (design system recommendations)

### 4.2 Enhanced audit gates with LLM review

Each audit gate gets an optional `--deep` flag that invokes an LLM with the relevant persona:

```
node scripts/audit.js --gate A --deep
```

- Gate A (deep): Document Analyst reviews `raw-extract.json` for completeness
- Gate B (deep): Style Cloner + Accessibility review `scraped-styles.json` for contrast, coherence
- Gate C (deep): Synthesizer + Content Architect review `merged-blueprint.json` for narrative flow
- Gate D (deep): Target Audience reviews built HTML for trust signals, clarity, CTA effectiveness

Without `--deep`, audits run deterministic checks only (current behavior, fast, free).

### 4.3 LLM integration approach

Use the Anthropic API (Claude) with:
- System prompt = persona markdown file contents
- User prompt = structured task + relevant data files
- Response = structured JSON (corrections, scores, recommendations)

Configuration in `whitelabel.config.js`:
```js
ai: {
  provider: "anthropic",       // or "openai", "local"
  model: "claude-sonnet-4-5-20250929",  // fast + capable
  enrichment: true,            // enable/disable LLM passes
  deepAudit: true,             // enable/disable LLM audit gates
}
```

This keeps the pipeline functional without API keys (deterministic-only mode) while enabling persona-powered quality when configured.

---

## Phase 5: Fix the Pipeline Scripts

**Goal:** `npm run generate -- --pdf deck.pdf --url https://example.com` actually works.

### 5.1 Fix Chrome dependency for scraping

Replace the hardcoded Chrome path:
```js
// Before
const CHROME_PATH = process.env.CHROME_PATH ||
  "/root/.cache/ms-playwright/chromium-1194/chrome-linux/chrome";

// After: auto-detect or use puppeteer's bundled browser
const CHROME_PATH = process.env.CHROME_PATH || findChromePath();
```

Add a `findChromePath()` utility that checks common locations, or switch to `puppeteer` (full, not `-core`) which bundles Chromium.

### 5.2 Add OCR fallback for image-based PDFs

When `pdf-parse` returns very short text (< 200 chars for a multi-page PDF), warn and offer `tesseract.js` as fallback:
```
Warning: PDF appears to be image-based (42 chars extracted from 18 pages).
Run with --ocr to attempt OCR extraction (requires tesseract.js).
```

### 5.3 Fix output directory references

`generate.js` line 96 says "Generated site is in dist/" — verify this matches `eleventy.config.js` output dir. Currently consistent (both use `dist/`), but add an assertion.

### 5.4 End-to-end test with the GEO 42 example

```bash
# Test the pipeline against the preserved example
cp examples/geo42/raw-extract.json data/
cp examples/geo42/scraped-styles.json data/
node scripts/merge.js
npm run build
node scripts/audit.js --gate ALL
```

This validates the pipeline works with known-good input and produces a buildable site.

---

## Phase 6: Documentation & Product Identity

### 6.1 README.md rewrite

Current README explains the GEO 42 site. New README should present DeckSiteAgent as a product:

```
# DeckSiteAgent

Multi-agent pipeline that turns a PDF deck + reference URL into a
production-ready static website.

## Quick Start
npm run generate -- --pdf your-deck.pdf --url https://reference-site.com

## How It Works
[pipeline diagram]

## Personas
11 specialized AI personas guide quality at every stage...

## Examples
See examples/geo42/ for a complete worked example.
```

### 6.2 AGENTS.md update

Update to reflect the actual integration model (persona-powered LLM enrichment) rather than the aspirational multi-agent framework. Be honest about what's deterministic vs LLM-powered.

### 6.3 Landing page for DeckSiteAgent Pages

The `alexroessner.github.io/DeckSiteAgent/` site should become a product demo page for the pipeline itself — not a copy of the GEO 42 site. This means the default `pageContent.json` and templates should describe DeckSiteAgent, the product.

---

## Implementation Order

| Step | Phase | What | Risk | MagUpSite Impact |
|------|-------|------|------|-----------------|
| 1 | 1.1 | Remove MagUpSite deploy from CI | None | Protects it |
| 2 | 1.2 | Remove hardcoded MagUpSite URLs | None | No impact (already deployed) |
| 3 | 1.3 | Simplify CI | None | No impact |
| 4 | 2.1 | Move GEO 42 data to examples/ | None | No impact |
| 5 | 2.2 | Create clean default data files | None | No impact |
| 6 | 3.1-3.7 | Genericize all templates | Medium (may break build) | No impact |
| 7 | 5.1-5.3 | Fix pipeline script issues | Low | No impact |
| 8 | 5.4 | End-to-end test | None | No impact |
| 9 | 4.1-4.3 | Wire persona LLM integration | Medium (new code) | No impact |
| 10 | 6.1-6.3 | Documentation rewrite | None | No impact |

Steps 1-3 should happen first and together. They're the safety net. Everything after is building the product with zero risk to the live site.

---

## What This Produces

After all phases:

- **MagUpSite** stays live forever, untouched, self-contained
- **DeckSiteAgent** is a standalone product where:
  - `npm run generate -- --pdf deck.pdf --url https://site.com` produces a full site
  - 11 personas are wired into the pipeline via LLM enrichment passes
  - Templates render any company's content generically
  - 4 audit gates run both deterministic checks and LLM-powered quality review
  - `examples/geo42/` proves it works with real-world data
  - `alexroessner.github.io/DeckSiteAgent/` showcases the product
